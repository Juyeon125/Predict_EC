{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import struct\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from adabound import AdaBound\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "file_dir = './Enzyme_Data/'\n",
    "file_dir_gz = './Enzyme_Data/gzfile/'\n",
    "#필요한 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \">21000b\"\n",
    "label_type = \">b\"\n",
    "\n",
    "val_length = 44622\n",
    "test_length = 44622\n",
    "train_length = 137099\n",
    "train_batch_size_value = 626 #데이터 크기 정하는곳\n",
    "test_batch_size_value = 603 #데이터 크기 정하는곳\n",
    "\n",
    "#train = 47, test = 67 이 나머지가 없는 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = \"deepec_prepare_train.idx3-ubyte\"\n",
    "train_label = \"deepec_prepare_train.idx1-ubyte\"\n",
    "\n",
    "test_seq = \"deepec_prepare_test.idx3-ubyte\"\n",
    "test_label = \"deepec_prepare_test.idx1-ubyte\"\n",
    "\n",
    "val_seq = \"deepec_prepare_val.idx3-ubyte\"\n",
    "val_label = \"deepec_prepare_val.idx1-ubyte\"\n",
    "\n",
    "def data_loader(length, seq_file_name, label_file_name, seq_type, label_type):\n",
    "    seq_data = open(file_dir + seq_file_name,'rb')\n",
    "    seq_data.seek(16)\n",
    "    \n",
    "    label_data = open(file_dir + label_file_name,'rb')\n",
    "    label_data.seek(8)\n",
    "    \n",
    "    while True:\n",
    "        seq = struct.unpack(data_type, seq_data.read(21000))\n",
    "        #1000 * 21 이기 때문에 21000byte씩 읽어 들임\n",
    "        seq = np.array(seq)\n",
    "        \n",
    "        seq = torch.FloatTensor(seq)\n",
    "        seq = torch.abs(seq)\n",
    "        #절대값을 사용한 이유는 -1의 값이 나오기 때문에 abs로 절대값을 취해서 1의 값으로 변환해서 사용\n",
    "        seq = seq.view(1,1000,21)\n",
    "        \n",
    "        label_value = struct.unpack(label_type, label_data.read(1))\n",
    "        label_value = np.array(label_value, dtype = float)\n",
    "        \n",
    "        #BCELoss는 output과 값의 크기;가 동일하여야 하기 때문에 아래와 같이 사용 \n",
    "        if label_value == 0:\n",
    "            label_value = torch.FloatTensor([1.0, 0.0])\n",
    "        else:\n",
    "            label_value = torch.FloatTensor([0.0, 1.0])\n",
    "            \n",
    "        yield seq, label_value #yield함수를 사용하여 데이터 한개씩 return\n",
    "\n",
    "#데이터 읽어 들이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(torch.utils.data.Dataset): \n",
    "    def __init__(self, length_t, seq_t, label_t):\n",
    "        train_data = data_loader(length_t, seq_t, label_t, data_type, label_type)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return train_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = next(train_data)\n",
    "        x = data[0]\n",
    "        y = data[1]    \n",
    "        return x, y\n",
    "    \n",
    "class ValDataset(torch.utils.data.Dataset): \n",
    "    def __init__(self, length_t, seq_t, label_t):\n",
    "        val_data = data_loader(length_t, seq_t, label_t, data_type, label_type)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return val_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = next(val_data)\n",
    "        x = data[0]\n",
    "        y = data[1]    \n",
    "        return x, y\n",
    "\n",
    "class TestDataset(torch.utils.data.Dataset): \n",
    "    def __init__(self, length_t, seq_t, label_t):\n",
    "        test_data = data_loader(length_t, seq_t, label_t, data_type, label_type)\n",
    "\n",
    "    def __len__(self):\n",
    "        return test_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = next(test_data)\n",
    "        x = data[0]\n",
    "        y = data[1]    \n",
    "        return x, y\n",
    "\n",
    "    \n",
    "\n",
    "train_data = data_loader(train_length, train_seq, train_label, data_type, label_type)\n",
    "test_data = data_loader(test_length, test_seq, test_label, data_type, label_type)\n",
    "val_data = data_loader(val_length, val_seq, val_label, data_type, label_type)\n",
    "\n",
    "train_dataset = TrainDataset(train_length, train_seq, train_label)\n",
    "val_dataset = ValDataset(val_length, val_seq, val_label)\n",
    "test_dataset = TestDataset(test_length, test_seq, test_label)\n",
    "\n",
    "traindataloader = DataLoader(train_dataset, batch_size = train_batch_size_value, shuffle = True, pin_memory = True, drop_last = True)\n",
    "testdataloader = DataLoader(test_dataset, batch_size = test_batch_size_value, pin_memory = True, drop_last = True)\n",
    "valdataloader = DataLoader(val_dataset, batch_size = test_batch_size_value, pin_memory = True, drop_last = True)\n",
    "\n",
    "#pin_memory = 학습 도중 고정 메모리를 사용하여 학습 시간 및 정확도 향상을 위해 사용\n",
    "#drop_last 는 배치사이즈만큼 사용시 데이터가 남는 현상이 있어 남는 데이터는 버리고 사용\n",
    "#customdataloader 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = [5.48951105, 0.55010511]\n",
    "#class_weight를 미리 계산한 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cuda() 붙여보기\n",
    "\n",
    "class CNN1(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 항상 torch.nn.Module을 상속받고 시작\n",
    "        super(CNN1, self).__init__()\n",
    "\n",
    "        eps_value = 1e-01\n",
    "        momentum_value = 0.99\n",
    "        bias_data = 0\n",
    "        #다음은 uniform_\n",
    "        conv1 = nn.Conv2d(1, 128, kernel_size = (4,21), stride = 1, dilation = 1) #케라스의 conv2d\n",
    "        nn.init.xavier_uniform_(conv1.weight) #케라스의 conv2d와 설정을 똑같이 하기위해 사용 \n",
    "        conv1.bias.data.fill_(bias_data) #케라스의 conv2d와 설정을 똑같이 하기위해 사용 \n",
    "        \n",
    "        batch_conv1 = nn.BatchNorm1d(128, momentum = momentum_value, eps = eps_value) #케라스의 batchnorm\n",
    "        pool1 = nn.MaxPool2d(kernel_size=(997,1))  #케라스의 MaxPool2d\n",
    "\n",
    "        self.CNN1_module = nn.Sequential(\n",
    "            conv1,\n",
    "            nn.ReLU(),\n",
    "            pool1,\n",
    "            nn.Flatten(),\n",
    "            batch_conv1\n",
    "        )\n",
    "        \n",
    "        conv2 = nn.Conv2d(1, 128, kernel_size = (8,21), stride = 1, dilation = 1)\n",
    "        nn.init.xavier_uniform_(conv2.weight) \n",
    "        conv2.bias.data.fill_(bias_data)\n",
    "        \n",
    "        batch_conv2 = nn.BatchNorm1d(128, momentum = momentum_value, eps = eps_value)\n",
    "        pool2 = nn.MaxPool2d(kernel_size=(993,1)) \n",
    "\n",
    "        self.CNN2_module = nn.Sequential(\n",
    "            conv2,\n",
    "            nn.ReLU(),\n",
    "            pool2,\n",
    "            nn.Flatten(),\n",
    "            batch_conv2\n",
    "        )\n",
    "        \n",
    "        conv3 = nn.Conv2d(1, 128, kernel_size = (16,21), stride = 1 , dilation = 1)\n",
    "        nn.init.xavier_uniform_(conv3.weight)\n",
    "        conv3.bias.data.fill_(bias_data)\n",
    "        \n",
    "        batch_conv3 = nn.BatchNorm1d(128, momentum = momentum_value, eps = eps_value)\n",
    "        pool3 = nn.MaxPool2d(kernel_size=(985,1)) \n",
    "    \n",
    "        self.CNN3_module = nn.Sequential(\n",
    "            conv3,\n",
    "            nn.ReLU(),\n",
    "            pool3,\n",
    "            nn.Flatten(),\n",
    "            batch_conv3\n",
    "        )\n",
    "\n",
    "        \n",
    "        fc1 = nn.Linear(384, 512) #케라스의 Dense\n",
    "        batch_fc1 = nn.BatchNorm1d(512, momentum = momentum_value, eps = eps_value)\n",
    "        nn.init.xavier_uniform_(fc1.weight) #케라스의 conv2d와 설정을 똑같이 하기위해 사용 \n",
    "        fc1.bias.data.fill_(bias_data) #케라스의 conv2d와 설정을 똑같이 하기위해 사용 \n",
    "\n",
    "        fc2 = nn.Linear(512, 512)\n",
    "        batch_fc2 = nn.BatchNorm1d(512, momentum = momentum_value, eps = eps_value)\n",
    "        nn.init.xavier_uniform_(fc2.weight)\n",
    "        fc2.bias.data.fill_(bias_data)\n",
    "\n",
    "        fc3 = nn.Linear(512, 2)\n",
    "        batch_fc3 = nn.BatchNorm1d(2, momentum = momentum_value, eps = eps_value)\n",
    "        nn.init.xavier_uniform_(fc3.weight)\n",
    "        fc3.bias.data.fill_(bias_data)\n",
    "        \n",
    " \n",
    "        self.fc_module = nn.Sequential(\n",
    "            fc1,\n",
    "            batch_fc1,\n",
    "            nn.ReLU(),\n",
    "            fc2,\n",
    "            batch_fc2,\n",
    "            nn.ReLU(),\n",
    "            fc3,\n",
    "            batch_fc3\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.CNN1_module(x) # @16*4*4\n",
    "        out2 = self.CNN2_module(x) # @16*4*4\n",
    "        out3 = self.CNN3_module(x) # @16*4*4\n",
    "        \n",
    "        out = torch.cat((out1, out2, out3), dim = 1) # torch.cat은 케라스의 Concatenate\n",
    "        \n",
    "        out = self.fc_module(out)\n",
    "        return out\n",
    "    \n",
    "#배치 정규화 균등분포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN1().cuda()\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight_value = torch.FloatTensor(class_weight).cuda()\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(weight = class_weight_value)\n",
    "\n",
    "learning_rate = 0.0002\n",
    "\n",
    "#optimizer = AdaBound(cnn.parameters(), lr = learning_rate, eps = 1e-08, final_lr=0.001)\n",
    "\n",
    "optimizer = optim.Adam(cnn.parameters(), lr = learning_rate, eps = 1e-08)#, weight_decay = 1e-01)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "train_loss_list = []\n",
    "test_loss_list = [] \n",
    "acc_list = [] \n",
    "non_acc_list = []\n",
    "enz_acc_list = []\n",
    "check_zero_pred_correct = 0\n",
    "train_total_list = []\n",
    "train_non_acc_list = []\n",
    "train_enz_acc_list = []\n",
    "\n",
    "for e in range(epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    test_loss = 0\n",
    "    \n",
    "    test_zero_count = 0\n",
    "    test_one_count = 0\n",
    "    \n",
    "    total_pred_correct = 0\n",
    "    one_pred_correct = 0\n",
    "    zero_pred_correct = 0\n",
    "    \n",
    "    train_zero_count = 0\n",
    "    train_one_count = 0\n",
    "    \n",
    "    train_total_pred_correct = 0\n",
    "    train_one_pred_correct = 0\n",
    "    train_zero_pred_correct = 0\n",
    "\n",
    "    train_data = data_loader(train_length, train_seq, train_label, data_type, label_type)\n",
    "    test_data = data_loader(test_length, test_seq, test_label, data_type, label_type)\n",
    "    val_data = data_loader(val_length, val_seq, val_label, data_type, label_type)\n",
    "    \n",
    "    for index, data in enumerate(traindataloader):\n",
    "        cnn.train()\n",
    "        input_data, label = data\n",
    "       \n",
    "        input_data = input_data.cuda(non_blocking = True)\n",
    "        trian_label = label.cuda(non_blocking = True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        model_output = cnn(input_data)\n",
    "\n",
    "        loss = criterion(model_output, trian_label)\n",
    "        loss.backward()          \n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        _, train_pred = torch.max(model_output.data, 1)\n",
    "        \n",
    "        for z in range(train_batch_size_value):\n",
    "            if trian_label[z][0] ==  1:\n",
    "                train_zero_count += 1\n",
    "                if train_pred[z] == 0:\n",
    "                    train_total_pred_correct += 1\n",
    "                    train_zero_pred_correct += 1\n",
    "                        \n",
    "            elif trian_label[z][0] ==  0:\n",
    "                train_one_count += 1\n",
    "                if train_pred[z] == 1:\n",
    "                    train_total_pred_correct += 1\n",
    "                    train_one_pred_correct += 1\n",
    "\n",
    "    with torch.no_grad(): # very very very very important!!!\n",
    "        \n",
    "        total_pred_correct = 0\n",
    "        zero_pred_correct = 0\n",
    "        one_pred_correct = 0\n",
    "        \n",
    "        for count, testdata in enumerate(testdataloader):\n",
    "            cnn.eval()\n",
    "            \n",
    "            input_data, test_labels = testdata\n",
    "            test_input_data = input_data.cuda(non_blocking = True)\n",
    "            test_labels = test_labels.cuda(non_blocking = True)\n",
    "            \n",
    "            test_outputs = cnn(test_input_data)\n",
    "\n",
    "            _, total_pred = torch.max(test_outputs.data, 1)\n",
    "            \n",
    "            loss = criterion(test_outputs, test_labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            for t in range(test_batch_size_value):\n",
    "                if test_labels[t][0] ==  1:\n",
    "                    test_zero_count += 1\n",
    "                    if total_pred[t] == 0:\n",
    "                        total_pred_correct += 1\n",
    "                        zero_pred_correct += 1\n",
    "                        \n",
    "                elif test_labels[t][0] ==  0:\n",
    "                    test_one_count += 1\n",
    "                    if total_pred[t] == 1:\n",
    "                        total_pred_correct += 1\n",
    "                        one_pred_correct += 1\n",
    "        \n",
    "        check_zero_correct = float(100 * zero_pred_correct) / test_zero_count\n",
    "        check_one_correct = float(100 * one_pred_correct) / test_one_count\n",
    "        \n",
    "        if check_zero_correct > 80:\n",
    "            if check_one_correct > 90:\n",
    "                print(\"model_save\")\n",
    "                torch.save(cnn.state_dict(), './saved_model/cnn1_adabound.pth')\n",
    "        \n",
    "        train_loss_list.append(train_loss/train_length)\n",
    "        test_loss_list.append(test_loss/test_length)\n",
    "        acc_list.append(float(100 * total_pred_correct) / test_length)\n",
    "        non_acc_list.append(float(100 * zero_pred_correct) / test_zero_count)\n",
    "        enz_acc_list.append(float(100 * one_pred_correct) / test_one_count)\n",
    "        \n",
    "        train_total_list.append(float(100 * train_total_pred_correct) / train_length)\n",
    "        train_non_acc_list.append(float(100 * train_zero_pred_correct) / train_zero_count)\n",
    "        train_enz_acc_list.append(float(100 * train_one_pred_correct) / train_one_count)\n",
    "        \n",
    "    print(\"epoch: {} | trn loss: {:.6f} | test loss: {:.6f}\".format(e+1, train_loss/train_length, test_loss / test_length))\n",
    "    print('Pred Total_acc : {:.2f} | Non_acc : {:.2f} | Enz : {:.2f}'.format(float(100 * total_pred_correct) / test_length, float(100 * zero_pred_correct) / test_zero_count, float(100 * one_pred_correct) / test_one_count))\n",
    "    print('train Total_acc : {:.2f} | Non_acc : {:.2f} | Enz : {:.2f}'.format(float(100 * train_total_pred_correct) / train_length, float(100 * train_zero_pred_correct) / train_zero_count, float(100 * train_one_pred_correct) / train_one_count))\n",
    "    print('작업 수행된 시간 : %f 초' % (time.time() - start_time))\n",
    "  \n",
    "    print(\"\")\n",
    "    \n",
    "# 학습 및 평가\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
