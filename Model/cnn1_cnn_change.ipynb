{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import struct\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from adabound import AdaBound\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "file_dir = './Enzyme_Data/'\n",
    "file_dir_gz = './Enzyme_Data/gzfile/'\n",
    "#필요한 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \">21000b\"\n",
    "\n",
    "val_length = 44622\n",
    "test_length = 44622\n",
    "train_length = 137099\n",
    "train_batch_size_value = 626 #데이터 크기 정하는곳\n",
    "test_batch_size_value = 603 #데이터 크기 정하는곳\n",
    "\n",
    "#train = 47, test = 67 이 나머지가 없는 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "13\n",
      "26\n",
      "35\n",
      "47\n",
      "73\n",
      "146\n",
      "219\n",
      "313\n",
      "438\n",
      "626\n",
      "939\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    if (137099 % (i + 1) < 6):\n",
    "        print(i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = \"deepec_prepare_train.idx3-ubyte\"\n",
    "train_label = \"deepec_prepare_train.idx1-ubyte\"\n",
    "\n",
    "test_seq = \"deepec_prepare_test.idx3-ubyte\"\n",
    "test_label = \"deepec_prepare_test.idx1-ubyte\"\n",
    "\n",
    "val_seq = \"deepec_prepare_val.idx3-ubyte\"\n",
    "val_label = \"deepec_prepare_val.idx1-ubyte\"\n",
    "\n",
    "def data_loader(length, seq_file_name, label_file_name):\n",
    "    img_f = open(file_dir + seq_file_name,'rb')\n",
    "    img_f.seek(16)\n",
    "    \n",
    "    lbl_f = open(file_dir + label_file_name,'rb')\n",
    "    lbl_f.seek(8)\n",
    "    \n",
    "    while True:\n",
    "        seq = struct.unpack(\">21000b\", img_f.read(21000))\n",
    "        #1000 * 21 이기 때문에 21000byte씩 읽어 들임\n",
    "        seq = np.array(seq, dtype = float)\n",
    "        \n",
    "        seq = torch.FloatTensor(seq)\n",
    "        seq = torch.abs(seq)\n",
    "        #절대값을 사용한 이유는 -1의 값이 나오기 때문에 abs로 절대값을 취해서 1의 값으로 변환해서 사용\n",
    "        seq = seq.view(1,1000,21)\n",
    "        \n",
    "        label_value = struct.unpack(\">b\", lbl_f.read(1))\n",
    "        label_value = torch.LongTensor(label_value)\n",
    "        \n",
    "        #BCELoss는 output과 값의 크기;가 동일하여야 하기 때문에 아래와 같이 사용 \n",
    "        if label_value == 0:\n",
    "            label_value = torch.FloatTensor([1.0, 0.0])\n",
    "        else:\n",
    "            label_value = torch.FloatTensor([0.0, 1.0])\n",
    "            \n",
    "        yield seq, label_value #yield함수를 사용하여 데이터 한개씩 return\n",
    "\n",
    "#데이터 읽어 들이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(torch.utils.data.Dataset): \n",
    "    def __init__(self, length, seq, label):\n",
    "        self.train_data = data_loader(length, seq, label)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return train_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = next(self.train_data)\n",
    "        x = data[0]\n",
    "        y = data[1]    \n",
    "        return x, y\n",
    "    \n",
    "class ValDataset(torch.utils.data.Dataset): \n",
    "    def __init__(self, length, seq, label):\n",
    "        self.val_data = data_loader(length, seq, label)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return val_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = next(self.val_data)\n",
    "        x = data[0]\n",
    "        y = data[1]    \n",
    "        return x, y\n",
    "\n",
    "class TestDataset(torch.utils.data.Dataset): \n",
    "    def __init__(self, length, seq, label):\n",
    "        self.test_data = data_loader(length, seq, label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return test_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = next(self.test_data)\n",
    "        x = data[0]\n",
    "        y = data[1]    \n",
    "        return x, y\n",
    "\n",
    "train_dataset = TrainDataset(train_length, train_seq, train_label)\n",
    "val_dataset = ValDataset(val_length, val_seq, val_label)\n",
    "test_dataset = TestDataset(test_length, test_seq, test_label)\n",
    "\n",
    "traindataloader = DataLoader(train_dataset, batch_size = train_batch_size_value, shuffle = True, pin_memory = True, drop_last = True)\n",
    "testdataloader = DataLoader(test_dataset, batch_size = test_batch_size_value, pin_memory = True, drop_last = True)\n",
    "valdataloader = DataLoader(val_dataset, batch_size = test_batch_size_value, pin_memory = True, drop_last = True)\n",
    "\n",
    "#pin_memory = 학습 도중 고정 메모리를 사용하여 학습 시간 및 정확도 향상을 위해 사용\n",
    "#drop_last 는 배치사이즈만큼 사용시 데이터가 남는 현상이 있어 남는 데이터는 버리고 사용\n",
    "#customdataloader 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = [5.48951105, 0.55010511]\n",
    "#class_weight를 미리 계산한 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cuda() 붙여보기\n",
    "\n",
    "class CNN1(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 항상 torch.nn.Module을 상속받고 시작\n",
    "        super(CNN1, self).__init__()\n",
    "\n",
    "        eps_value = 1e-01\n",
    "        momentum_value = 0.99\n",
    "        bias_data = 10\n",
    "        #다음은 uniform_\n",
    "        conv1 = nn.Conv2d(1, 128, kernel_size = (4,21), stride = 1, dilation = 1) #케라스의 conv2d\n",
    "        nn.init.xavier_uniform_(conv1.weight) #케라스의 conv2d와 설정을 똑같이 하기위해 사용 \n",
    "        conv1.bias.data.fill_(bias_data) #케라스의 conv2d와 설정을 똑같이 하기위해 사용 \n",
    "        \n",
    "        batch_conv1 = nn.BatchNorm1d(128, momentum = momentum_value, eps = eps_value) #케라스의 batchnorm\n",
    "        pool1 = nn.MaxPool2d(kernel_size=(997,1))  #케라스의 MaxPool2d\n",
    "\n",
    "        self.CNN1_module = nn.Sequential(\n",
    "            conv1,\n",
    "            nn.ReLU(),\n",
    "            pool1,\n",
    "            nn.Flatten(),\n",
    "            batch_conv1\n",
    "        )\n",
    "        \n",
    "        conv2 = nn.Conv2d(1, 128, kernel_size = (8,21), stride = 1, dilation = 1)\n",
    "        nn.init.xavier_uniform_(conv2.weight) \n",
    "        conv2.bias.data.fill_(bias_data)\n",
    "        \n",
    "        batch_conv2 = nn.BatchNorm1d(128, momentum = momentum_value, eps = eps_value)\n",
    "        pool2 = nn.MaxPool2d(kernel_size=(993,1)) \n",
    "\n",
    "        self.CNN2_module = nn.Sequential(\n",
    "            conv2,\n",
    "            nn.ReLU(),\n",
    "            pool2,\n",
    "            nn.Flatten(),\n",
    "            batch_conv2\n",
    "        )\n",
    "        \n",
    "        conv3 = nn.Conv2d(1, 128, kernel_size = (16,21), stride = 1 , dilation = 1)\n",
    "        nn.init.xavier_uniform_(conv3.weight)\n",
    "        conv3.bias.data.fill_(bias_data)\n",
    "        \n",
    "        batch_conv3 = nn.BatchNorm1d(128, momentum = momentum_value, eps = eps_value)\n",
    "        pool3 = nn.MaxPool2d(kernel_size=(985,1)) \n",
    "    \n",
    "        self.CNN3_module = nn.Sequential(\n",
    "            conv3,\n",
    "            nn.ReLU(),\n",
    "            pool3,\n",
    "            nn.Flatten(),\n",
    "            batch_conv3\n",
    "        )\n",
    "\n",
    "        \n",
    "        fc1 = nn.Linear(384, 512) #케라스의 Dense\n",
    "        batch_fc1 = nn.BatchNorm1d(512, momentum = momentum_value, eps = eps_value)\n",
    "        nn.init.xavier_uniform_(fc1.weight) #케라스의 conv2d와 설정을 똑같이 하기위해 사용 \n",
    "        fc1.bias.data.fill_(bias_data) #케라스의 conv2d와 설정을 똑같이 하기위해 사용 \n",
    "\n",
    "        fc2 = nn.Linear(512, 512)\n",
    "        batch_fc2 = nn.BatchNorm1d(512, momentum = momentum_value, eps = eps_value)\n",
    "        nn.init.xavier_uniform_(fc2.weight)\n",
    "        fc2.bias.data.fill_(bias_data)\n",
    "\n",
    "        fc3 = nn.Linear(512, 2)\n",
    "        batch_fc3 = nn.BatchNorm1d(2, momentum = momentum_value, eps = eps_value)\n",
    "        nn.init.xavier_uniform_(fc3.weight)\n",
    "        fc3.bias.data.fill_(bias_data)\n",
    "        \n",
    " \n",
    "        self.fc_module = nn.Sequential(\n",
    "            fc1,\n",
    "            batch_fc1,\n",
    "            nn.ReLU(),\n",
    "            fc2,\n",
    "            batch_fc2,\n",
    "            nn.ReLU(),\n",
    "            fc3,\n",
    "            batch_fc3\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.CNN1_module(x) # @16*4*4\n",
    "        out2 = self.CNN2_module(x) # @16*4*4\n",
    "        out3 = self.CNN3_module(x) # @16*4*4\n",
    "        \n",
    "        out = torch.cat((out1, out2, out3), dim = 1) # torch.cat은 케라스의 Concatenate\n",
    "        \n",
    "        out = self.fc_module(out)\n",
    "        return out\n",
    "    \n",
    "#배치 정규화 균등분포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN1().cuda()\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "unpack requires a buffer of 1 bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-1e10aa34c5e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mtrain_zero_pred_correct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraindataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0minput_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-8245e04b69e3>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-6cf8ce15ea4a>\u001b[0m in \u001b[0;36mdata_loader\u001b[1;34m(length, seq_file_name, label_file_name)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mseq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m21\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mlabel_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\">b\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlbl_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mlabel_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: unpack requires a buffer of 1 bytes"
     ]
    }
   ],
   "source": [
    "class_weight_value = torch.FloatTensor(class_weight).cuda()\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(weight = class_weight_value)\n",
    "\n",
    "learning_rate = 0.0002\n",
    "\n",
    "#optimizer = AdaBound(cnn.parameters(), lr = learning_rate, eps = 1e-08, final_lr=0.001)\n",
    "\n",
    "optimizer = optim.Adam(cnn.parameters(), lr = learning_rate, eps = 1e-08)#, weight_decay = 1e-01)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "train_loss_list = []\n",
    "test_loss_list = [] \n",
    "acc_list = [] \n",
    "non_acc_list = []\n",
    "enz_acc_list = []\n",
    "check_zero_pred_correct = 0\n",
    "train_total_list = []\n",
    "train_non_acc_list = []\n",
    "train_enz_acc_list = []\n",
    "\n",
    "for e in range(epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    test_loss = 0\n",
    "    \n",
    "    test_zero_count = 0\n",
    "    test_one_count = 0\n",
    "    \n",
    "    total_pred_correct = 0\n",
    "    one_pred_correct = 0\n",
    "    zero_pred_correct = 0\n",
    "    \n",
    "    train_zero_count = 0\n",
    "    train_one_count = 0\n",
    "    \n",
    "    train_total_pred_correct = 0\n",
    "    train_one_pred_correct = 0\n",
    "    train_zero_pred_correct = 0\n",
    "\n",
    "    for index, data in enumerate(traindataloader):\n",
    "        cnn.train()\n",
    "        input_data, label = data\n",
    "       \n",
    "        input_data = input_data.cuda(non_blocking = True)\n",
    "        #label = label.view(-1)\n",
    "        trian_label = label.cuda(non_blocking = True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        model_output = cnn(input_data)\n",
    "        \n",
    "        #logsoftmax = nn.LogSoftmax()\n",
    "        #loss = criterion(logsoftmax(model_output), train_label)\n",
    "        \n",
    "        loss = criterion(model_output, trian_label)\n",
    "        loss.backward()          \n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        _, train_pred = torch.max(model_output.data, 1)\n",
    "        \n",
    "        for z in range(train_batch_size_value):\n",
    "            if trian_label[z][0] ==  1:\n",
    "                train_zero_count += 1\n",
    "                if train_pred[z] == 0:\n",
    "                    train_total_pred_correct += 1\n",
    "                    train_zero_pred_correct += 1\n",
    "                        \n",
    "            elif trian_label[z][0] ==  0:\n",
    "                train_one_count += 1\n",
    "                if train_pred[z] == 1:\n",
    "                    train_total_pred_correct += 1\n",
    "                    train_one_pred_correct += 1\n",
    "\n",
    "    with torch.no_grad(): # very very very very important!!!\n",
    "        \n",
    "        total_pred_correct = 0\n",
    "        zero_pred_correct = 0\n",
    "        one_pred_correct = 0\n",
    "        \n",
    "        for count, testdata in enumerate(testdataloader):\n",
    "            cnn.eval()\n",
    "            \n",
    "            input_data, test_labels = testdata\n",
    "            test_input_data = input_data.cuda(non_blocking = True)\n",
    "            test_labels = test_labels.cuda(non_blocking = True)\n",
    "            \n",
    "            test_outputs = cnn(test_input_data)\n",
    "\n",
    "            _, total_pred = torch.max(test_outputs.data, 1)\n",
    "            \n",
    "            loss = criterion(test_outputs, test_labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            for t in range(test_batch_size_value):\n",
    "                if test_labels[t][0] ==  1:\n",
    "                    test_zero_count += 1\n",
    "                    if total_pred[t] == 0:\n",
    "                        total_pred_correct += 1\n",
    "                        zero_pred_correct += 1\n",
    "                        \n",
    "                elif test_labels[t][0] ==  0:\n",
    "                    test_one_count += 1\n",
    "                    if total_pred[t] == 1:\n",
    "                        total_pred_correct += 1\n",
    "                        one_pred_correct += 1\n",
    "        \n",
    "        check_zero_correct = float(100 * zero_pred_correct) / test_zero_count\n",
    "        check_one_correct = float(100 * one_pred_correct) / test_one_count\n",
    "        \n",
    "        if check_zero_correct > 80:\n",
    "            if check_one_correct > 90:\n",
    "                print(\"model_save\")\n",
    "                torch.save(cnn.state_dict(), './saved_model/cnn1_adabound.pth')\n",
    "        \n",
    "        train_loss_list.append(train_loss/train_length)\n",
    "        test_loss_list.append(test_loss/test_length)\n",
    "        acc_list.append(float(100 * total_pred_correct) / test_length)\n",
    "        non_acc_list.append(float(100 * zero_pred_correct) / test_zero_count)\n",
    "        enz_acc_list.append(float(100 * one_pred_correct) / test_one_count)\n",
    "        \n",
    "        train_total_list.append(float(100 * train_total_pred_correct) / train_length)\n",
    "        train_non_acc_list.append(float(100 * train_zero_pred_correct) / train_zero_count)\n",
    "        train_enz_acc_list.append(float(100 * train_one_pred_correct) / train_one_count)\n",
    "        \n",
    "    print(\"epoch: {} | trn loss: {:.6f} | test loss: {:.6f}\".format(e+1, train_loss/train_length, test_loss / test_length))\n",
    "    print('Pred Total_acc : {:.2f} | Non_acc : {:.2f} | Enz : {:.2f}'.format(float(100 * total_pred_correct) / test_length, float(100 * zero_pred_correct) / test_zero_count, float(100 * one_pred_correct) / test_one_count))\n",
    "    print('train Total_acc : {:.2f} | Non_acc : {:.2f} | Enz : {:.2f}'.format(float(100 * train_total_pred_correct) / train_length, float(100 * train_zero_pred_correct) / train_zero_count, float(100 * train_one_pred_correct) / train_one_count))\n",
    "    print('작업 수행된 시간 : %f 초' % (time.time() - start_time))\n",
    "  \n",
    "    print(\"\")\n",
    "    \n",
    "# 학습 및 평가\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([626, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1252])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trian_label.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cuda() 붙여보기\n",
    "\n",
    "class CNN1(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 항상 torch.nn.Module을 상속받고 시작\n",
    "        super(CNN1, self).__init__()\n",
    "\n",
    "        eps_value = 1e-01\n",
    "        momentum_value = 0.99\n",
    "        bias_data = 0\n",
    "        batch_test = torch.FloatTensor(128,128)\n",
    "        batch_data = nn.init.xavier_normal_(batch_test)\n",
    "        \n",
    "        bias_data_two = 0\n",
    "        \n",
    "        conv0 = nn.Conv2d(1, 128, kernel_size = (2,21), stride = 1, dilation = 1) #케라스의 conv2d\n",
    "        nn.init.xavier_normal_(conv0.weight) #케라스의 conv2d와 설정을 똑같이 하기위해 사용 \n",
    "        conv0.bias.data.fill_(bias_data) #케라스의 conv2d와 설정을 똑같이 하기위해 사용 \n",
    "        \n",
    "        batch_conv0 = nn.BatchNorm2d(128, momentum = momentum_value, eps = eps_value) #케라스의 batchnorm\n",
    "        pool0 = nn.MaxPool2d(kernel_size=(999,1))  #케라스의 MaxPool2d\n",
    "      \n",
    "        self.CNN0_module = nn.Sequential(\n",
    "            conv0,\n",
    "            batch_conv0,\n",
    "            nn.ReLU(),\n",
    "            pool0\n",
    "        )\n",
    "        \n",
    "        batch_conv0.weight.data = batch_data[0]\n",
    "        batch_conv0.bias.data.fill_(bias_data_two)\n",
    "        \n",
    "        conv1 = nn.Conv2d(1, 128, kernel_size = (4,21), stride = 1, dilation = 1) #케라스의 conv2d\n",
    "        nn.init.xavier_normal_(conv1.weight) #케라스의 conv2d와 설정을 똑같이 하기위해 사용 \n",
    "        \n",
    "        conv1.bias.data.fill_(bias_data) #케라스의 conv2d와 설정을 똑같이 하기위해 사용 \n",
    "        \n",
    "        batch_conv1 = nn.BatchNorm2d(128, momentum = momentum_value, eps = eps_value) #케라스의 batchnorm\n",
    "\n",
    "        pool1 = nn.MaxPool2d(kernel_size=(997,1))  #케라스의 MaxPool2d\n",
    "        \n",
    "        batch_conv1.weight.data = batch_data[0]\n",
    "        batch_conv1.bias.data.fill_(bias_data_two)  \n",
    "        \n",
    "        self.CNN1_module = nn.Sequential(\n",
    "            conv1,\n",
    "            batch_conv1,\n",
    "            nn.ReLU(),\n",
    "            pool1\n",
    "        )\n",
    "        \n",
    "        conv2 = nn.Conv2d(1, 128, kernel_size = (8,21), stride = 1, dilation = 1)\n",
    "        nn.init.xavier_normal_(conv2.weight) \n",
    "        conv2.bias.data.fill_(bias_data)\n",
    "        \n",
    "        batch_conv2 = nn.BatchNorm2d(128, momentum = momentum_value, eps = eps_value)\n",
    "        pool2 = nn.MaxPool2d(kernel_size=(993,1)) \n",
    "        \n",
    "        batch_conv2.weight.data = batch_data[0]\n",
    "        batch_conv2.bias.data.fill_(bias_data_two)\n",
    "        \n",
    "        self.CNN2_module = nn.Sequential(\n",
    "            conv2,\n",
    "            batch_conv2,\n",
    "            nn.ReLU(),\n",
    "            pool2\n",
    "        )\n",
    "        \n",
    "        conv3 = nn.Conv2d(1, 128, kernel_size = (16,21), stride = 1 , dilation = 1)\n",
    "        nn.init.xavier_normal_(conv3.weight)\n",
    "        conv3.bias.data.fill_(bias_data)\n",
    "        \n",
    "        batch_conv3 = nn.BatchNorm2d(128, momentum = momentum_value, eps = eps_value)\n",
    "        pool3 = nn.MaxPool2d(kernel_size=(985,1)) \n",
    "        \n",
    "        batch_conv3.weight.data = batch_data[0]\n",
    "        batch_conv3.bias.data.fill_(bias_data_two)\n",
    "        \n",
    "        self.CNN3_module = nn.Sequential(\n",
    "            conv3,\n",
    "            batch_conv3,\n",
    "            nn.ReLU(),\n",
    "            pool3\n",
    "        )\n",
    "        \n",
    "        conv4 = nn.Conv2d(1, 128, kernel_size = (21,21), stride = 1 , dilation = 1)\n",
    "        nn.init.xavier_normal_(conv4.weight)\n",
    "        conv4.bias.data.fill_(bias_data)\n",
    "        \n",
    "        batch_conv4 = nn.BatchNorm2d(128, momentum = momentum_value, eps = eps_value)\n",
    "        pool4 = nn.MaxPool2d(kernel_size=(980,1)) \n",
    "        \n",
    "        batch_conv4.weight.data = batch_data[0]\n",
    "        batch_conv4.bias.data.fill_(bias_data)\n",
    "        \n",
    "        self.CNN4_module = nn.Sequential(\n",
    "            conv4,\n",
    "            batch_conv4,\n",
    "            nn.ReLU(),\n",
    "            pool4\n",
    "        )\n",
    "        \n",
    "        \n",
    "        batch_test_512 = torch.FloatTensor(512,512)\n",
    "        batch_data_512 = nn.init.xavier_normal_(batch_test_512)\n",
    "        \n",
    "        batch_test_2 = torch.FloatTensor(2,2)\n",
    "        batch_data_2 = nn.init.xavier_normal_(batch_test_2)\n",
    "        \n",
    "        \n",
    "        fc1 = nn.Linear(384, 512) #케라스의 Dense\n",
    "        batch_fc1 = nn.BatchNorm1d(512, momentum = momentum_value, eps = eps_value)\n",
    "        nn.init.xavier_normal_(fc1.weight) #케라스의 conv2d와 설정을 똑같이 하기위해 사용 \n",
    "        fc1.bias.data.fill_(bias_data) #케라스의 conv2d와 설정을 똑같이 하기위해 사용 \n",
    "        \n",
    "        batch_fc1.weight.data = batch_data_512[0]\n",
    "        batch_fc1.bias.data.fill_(bias_data_two)\n",
    "        \n",
    "        fc2 = nn.Linear(512, 512)\n",
    "        batch_fc2 = nn.BatchNorm1d(512, momentum = momentum_value, eps = eps_value)\n",
    "        nn.init.xavier_normal_(fc2.weight)\n",
    "        fc2.bias.data.fill_(bias_data)\n",
    "        \n",
    "        batch_fc2.weight.data = batch_data_512[0]\n",
    "        batch_fc2.bias.data.fill_(bias_data_two)\n",
    "        \n",
    "        fc3 = nn.Linear(384, 2)\n",
    "        batch_fc3 = nn.BatchNorm1d(2, momentum = momentum_value, eps = eps_value)\n",
    "        nn.init.xavier_normal_(fc3.weight)\n",
    "        fc3.bias.data.fill_(bias_data)\n",
    "        \n",
    "        batch_fc3.weight.data = batch_data_2[0]\n",
    "        batch_fc3.bias.data.fill_(bias_data_two)\n",
    "   \n",
    "        self.fc_module = nn.Sequential(\n",
    "            fc1,\n",
    "            batch_fc1,\n",
    "            nn.ReLU(),\n",
    "            fc2,\n",
    "            batch_fc2,\n",
    "            nn.ReLU(),\n",
    "            fc3,\n",
    "            batch_fc3\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #out0 = self.CNN0_module(x) # @16*4*4\n",
    "        out1 = self.CNN1_module(x) # @16*4*4\n",
    "        out2 = self.CNN2_module(x) # @16*4*4\n",
    "        out3 = self.CNN3_module(x) # @16*4*4\n",
    "        #out4 = self.CNN4_module(x) # @16*4*4\n",
    "        # make linear\n",
    "        #print(self.batch_con12.weight.size())\n",
    "        #print(self.batch_con12.bias)\n",
    "        \n",
    "        #ut = torch.cat((out0, out1, out2, out3, out4), dim = 1) # torch.cat은 케라스의 Concatenate\n",
    "        out = torch.cat((out1, out2, out3), dim = 1) # torch.cat은 케라스의 Concatenate\n",
    "        #out = torch.cat((out1, out2), dim = 1) # torch.cat은 케라스의 Concatenate\n",
    "        #out = out1\n",
    "        dim = 1\n",
    "        \n",
    "        for d in out.size()[1:]: # 최종 갯수를 알기위한 for문\n",
    "            dim = dim * d\n",
    "        \n",
    "        \n",
    "        out = out.view(-1, dim)\n",
    "        out = self.fc_module(out)\n",
    "        return out\n",
    "    \n",
    "#배치 정규화 균등분포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cuda() 붙여보기\n",
    "\n",
    "class CNN1(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 항상 torch.nn.Module을 상속받고 시작\n",
    "        super(CNN1, self).__init__()\n",
    "\n",
    "        eps_value = 1e-01\n",
    "        momentum_value = 0.99\n",
    "        bias_data = 0\n",
    "        batch_test = torch.FloatTensor(128,128)\n",
    "        batch_data = nn.init.xavier_normal_(batch_test)\n",
    "        \n",
    "        bias_data_two = 0\n",
    "        \n",
    "        conv0 = nn.Conv2d(1, 128, kernel_size = (2,21), stride = 1, dilation = 1) #케라스의 conv2d\n",
    "        nn.init.xavier_normal_(conv0.weight) #케라스의 conv2d와 설정을 똑같이 하기위해 사용 \n",
    "        conv0.bias.data.fill_(bias_data) #케라스의 conv2d와 설정을 똑같이 하기위해 사용 \n",
    "        \n",
    "        batch_conv0 = nn.BatchNorm2d(128, momentum = momentum_value, eps = eps_value) #케라스의 batchnorm\n",
    "        pool0 = nn.MaxPool2d(kernel_size=(999,1))  #케라스의 MaxPool2d\n",
    "      \n",
    "        self.CNN0_module = nn.Sequential(\n",
    "            conv0,\n",
    "            batch_conv0,\n",
    "            nn.ReLU(),\n",
    "            pool0\n",
    "        )\n",
    "        \n",
    "        batch_conv0.weight.data = batch_data[0]\n",
    "        batch_conv0.bias.data.fill_(bias_data_two)\n",
    "        \n",
    "        conv1 = nn.Conv2d(1, 128, kernel_size = (4,21), stride = 1, dilation = 1) #케라스의 conv2d\n",
    "        nn.init.xavier_normal_(conv1.weight) #케라스의 conv2d와 설정을 똑같이 하기위해 사용 \n",
    "        \n",
    "        conv1.bias.data.fill_(bias_data) #케라스의 conv2d와 설정을 똑같이 하기위해 사용 \n",
    "        \n",
    "        batch_conv1 = nn.BatchNorm2d(128, momentum = momentum_value, eps = eps_value) #케라스의 batchnorm\n",
    "\n",
    "        pool1 = nn.MaxPool2d(kernel_size=(997,1))  #케라스의 MaxPool2d\n",
    "        \n",
    "        batch_conv1.weight.data = batch_data[0]\n",
    "        batch_conv1.bias.data.fill_(bias_data_two)  \n",
    "        \n",
    "        self.CNN1_module = nn.Sequential(\n",
    "            conv1,\n",
    "            batch_conv1,\n",
    "            nn.ReLU(),\n",
    "            pool1\n",
    "        )\n",
    "        \n",
    "        conv2 = nn.Conv2d(1, 128, kernel_size = (8,21), stride = 1, dilation = 1)\n",
    "        nn.init.xavier_normal_(conv2.weight) \n",
    "        conv2.bias.data.fill_(bias_data)\n",
    "        \n",
    "        batch_conv2 = nn.BatchNorm2d(128, momentum = momentum_value, eps = eps_value)\n",
    "        pool2 = nn.MaxPool2d(kernel_size=(993,1)) \n",
    "        \n",
    "        batch_conv2.weight.data = batch_data[0]\n",
    "        batch_conv2.bias.data.fill_(bias_data_two)\n",
    "        \n",
    "        self.CNN2_module = nn.Sequential(\n",
    "            conv2,\n",
    "            batch_conv2,\n",
    "            nn.ReLU(),\n",
    "            pool2\n",
    "        )\n",
    "        \n",
    "        conv3 = nn.Conv2d(1, 128, kernel_size = (16,21), stride = 1 , dilation = 1)\n",
    "        nn.init.xavier_normal_(conv3.weight)\n",
    "        conv3.bias.data.fill_(bias_data)\n",
    "        \n",
    "        batch_conv3 = nn.BatchNorm2d(128, momentum = momentum_value, eps = eps_value)\n",
    "        pool3 = nn.MaxPool2d(kernel_size=(985,1)) \n",
    "        \n",
    "        batch_conv3.weight.data = batch_data[0]\n",
    "        batch_conv3.bias.data.fill_(bias_data_two)\n",
    "        \n",
    "        self.CNN3_module = nn.Sequential(\n",
    "            conv3,\n",
    "            batch_conv3,\n",
    "            nn.ReLU(),\n",
    "            pool3\n",
    "        )\n",
    "        \n",
    "        conv4 = nn.Conv2d(1, 128, kernel_size = (21,21), stride = 1 , dilation = 1)\n",
    "        nn.init.xavier_normal_(conv4.weight)\n",
    "        conv4.bias.data.fill_(bias_data)\n",
    "        \n",
    "        batch_conv4 = nn.BatchNorm2d(128, momentum = momentum_value, eps = eps_value)\n",
    "        pool4 = nn.MaxPool2d(kernel_size=(980,1)) \n",
    "        \n",
    "        batch_conv4.weight.data = batch_data[0]\n",
    "        batch_conv4.bias.data.fill_(bias_data)\n",
    "        \n",
    "        self.CNN4_module = nn.Sequential(\n",
    "            conv4,\n",
    "            batch_conv4,\n",
    "            nn.ReLU(),\n",
    "            pool4\n",
    "        )\n",
    "        \n",
    "        \n",
    "        batch_test_512 = torch.FloatTensor(512,512)\n",
    "        batch_data_512 = nn.init.xavier_normal_(batch_test_512)\n",
    "        \n",
    "        batch_test_2 = torch.FloatTensor(2,2)\n",
    "        batch_data_2 = nn.init.xavier_normal_(batch_test_2)\n",
    "        \n",
    "        \n",
    "        fc3 = nn.Linear(384, 2)\n",
    "        batch_fc3 = nn.BatchNorm1d(2, momentum = momentum_value, eps = eps_value)\n",
    "        nn.init.xavier_normal_(fc3.weight)\n",
    "        fc3.bias.data.fill_(bias_data)\n",
    "        \n",
    "        batch_fc3.weight.data = batch_data_2[0]\n",
    "        batch_fc3.bias.data.fill_(bias_data_two)\n",
    "        \n",
    "        self.fc_module = nn.Sequential(\n",
    "            fc3,\n",
    "            batch_fc3\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #out0 = self.CNN0_module(x) # @16*4*4\n",
    "        out1 = self.CNN1_module(x) # @16*4*4\n",
    "        out2 = self.CNN2_module(x) # @16*4*4\n",
    "        out3 = self.CNN3_module(x) # @16*4*4\n",
    "        #out4 = self.CNN4_module(x) # @16*4*4\n",
    "        # make linear\n",
    "        #print(self.batch_con12.weight.size())\n",
    "        #print(self.batch_con12.bias)\n",
    "        \n",
    "        #ut = torch.cat((out0, out1, out2, out3, out4), dim = 1) # torch.cat은 케라스의 Concatenate\n",
    "        out = torch.cat((out1, out2, out3), dim = 1) # torch.cat은 케라스의 Concatenate\n",
    "        #out = torch.cat((out1, out2), dim = 1) # torch.cat은 케라스의 Concatenate\n",
    "        #out = out1\n",
    "        dim = 1\n",
    "        \n",
    "        for d in out.size()[1:]: # 최종 갯수를 알기위한 for문\n",
    "            dim = dim * d\n",
    "        \n",
    "        \n",
    "        out = out.view(-1, dim)\n",
    "        out = self.fc_module(out)\n",
    "        return out\n",
    "    \n",
    "#배치 정규화 균등분포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
