{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import struct\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "file_dir = './Enzyme_Data/'\n",
    "file_dir_gz = './Enzyme_Data/gzfile/'\n",
    "#필요한 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \">21000b\"\n",
    "\n",
    "val_length = 44622\n",
    "test_length = 44622\n",
    "train_length = 137099\n",
    "train_batch_size_value = 47 #데이터 크기 정하는곳\n",
    "test_batch_size_value = 67 #데이터 크기 정하는곳\n",
    "\n",
    "#train = 47, test = 67 이 나머지가 없는 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = \"deepec_prepare_train.idx3-ubyte\"\n",
    "train_label = \"deepec_prepare_train.idx1-ubyte\"\n",
    "\n",
    "test_seq = \"deepec_prepare_test.idx3-ubyte\"\n",
    "test_label = \"deepec_prepare_test.idx1-ubyte\"\n",
    "\n",
    "val_seq = \"deepec_prepare_val.idx3-ubyte\"\n",
    "val_label = \"deepec_prepare_val.idx1-ubyte\"\n",
    "\n",
    "def data_loader(length, seq_file_name, label_file_name):\n",
    "    img_f = open(file_dir + seq_file_name,'rb')\n",
    "    img_signature , img_count, row_count, columns_count = struct.unpack(\">IIII\", img_f.read(16))\n",
    "    \n",
    "    lbl_f = open(file_dir + label_file_name,'rb')\n",
    "    lb_signature, lbl_count = struct.unpack(\">II\", lbl_f.read(8))\n",
    "    \n",
    "    for i in range(length):\n",
    "        seq = struct.unpack(\">21000b\", img_f.read(21000))\n",
    "        #1000 * 21 이기 때문에 21000byte씩 읽어 들임\n",
    "        seq = np.array(seq, dtype = float)\n",
    "        \n",
    "        seq = torch.FloatTensor(seq)\n",
    "        seq = torch.abs(seq)\n",
    "        #절대값을 사용한 이유는 -1의 값이 나오기 때문에 abs로 절대값을 취해서 1의 값으로 변환해서 사용\n",
    "        seq = seq.view(1,1000,21)\n",
    "        \n",
    "        label_value = struct.unpack(\">b\", lbl_f.read(1))\n",
    "        label_value = np.array(label_value, dtype = float)\n",
    "        \n",
    "        #BCELoss는 output과 값의 크기;가 동일하여야 하기 때문에 아래와 같이 사용 \n",
    "        if label_value == 0:\n",
    "            label_value = torch.FloatTensor([1.0, 0.0])\n",
    "        else:\n",
    "            label_value = torch.FloatTensor([0.0, 1.0])\n",
    "            \n",
    "        yield seq, label_value #yield함수를 사용하여 데이터 한개씩 return\n",
    "\n",
    "train_data = data_loader(train_length, train_seq, train_label)\n",
    "test_data = data_loader(test_length, test_seq, test_label)\n",
    "val_data = data_loader(val_length, val_seq, val_label)\n",
    "\n",
    "#데이터 읽어 들이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(torch.utils.data.Dataset): \n",
    "    def __init__(self):\n",
    "        train_data = data_loader(train_length, train_seq, train_label)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return train_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = next(train_data)\n",
    "        x = data[0]\n",
    "        y = data[1]    \n",
    "        return x, y\n",
    "    \n",
    "class ValDataset(torch.utils.data.Dataset): \n",
    "    def __init__(self):\n",
    "        val_data = data_loader(val_length, val_seq, val_label)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return val_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = next(val_data)\n",
    "        x = data[0]\n",
    "        y = data[1]    \n",
    "        return x, y\n",
    "\n",
    "class TestDataset(torch.utils.data.Dataset): \n",
    "    def __init__(self):\n",
    "        test_data = data_loader(test_length, test_seq, test_label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return test_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = next(test_data)\n",
    "        x = data[0]\n",
    "        y = data[1]    \n",
    "        return x, y\n",
    "\n",
    "train_dataset = TrainDataset()\n",
    "val_dataset = ValDataset()\n",
    "test_dataset = TestDataset()\n",
    "\n",
    "traindataloader = DataLoader(train_dataset, batch_size = train_batch_size_value, shuffle = True, pin_memory = True, drop_last = True)\n",
    "testdataloader = DataLoader(test_dataset, batch_size = test_batch_size_value, shuffle = True, pin_memory = True, drop_last = True)\n",
    "valdataloader = DataLoader(val_dataset, batch_size = test_batch_size_value, shuffle = True, pin_memory = True, drop_last = True)\n",
    "\n",
    "#pin_memory = 학습 도중 고정 메모리를 사용하여 학습 시간 및 정확도 향상을 위해 사용\n",
    "#drop_last 는 배치사이즈만큼 사용시 데이터가 남는 현상이 있어 남는 데이터는 버리고 사용\n",
    "#customdataloader 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = [5.48951105, 0.55010511]\n",
    "#class_weight를 미리 계산한 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cuda() 붙여보기\n",
    "\n",
    "class CNN1(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 항상 torch.nn.Module을 상속받고 시작\n",
    "        super(CNN1, self).__init__()\n",
    "\n",
    "        eps_value = 1e-01\n",
    "        momentum_value = 0.99\n",
    "        bias_data = -10\n",
    "        conv1 = nn.Conv2d(1, 128, kernel_size = (4,21), stride = 1, dilation = 1) #케라스의 conv2d\n",
    "        nn.init.xavier_uniform_(conv1.weight) #케라스의 conv2d와 설정을 똑같이 하기위해 사용 \n",
    "        conv1.bias.data.fill_(bias_data) #케라스의 conv2d와 설정을 똑같이 하기위해 사용 \n",
    "        \n",
    "        batch_conv1 = nn.BatchNorm2d(128, momentum = momentum_value, eps = eps_value) #케라스의 batchnorm\n",
    "        pool1 = nn.MaxPool2d(kernel_size=(997,1))  #케라스의 MaxPool2d\n",
    "      \n",
    "        self.CNN1_module = nn.Sequential(\n",
    "            conv1,\n",
    "            batch_conv1,\n",
    "            nn.ReLU(),\n",
    "            pool1\n",
    "        )\n",
    "        \n",
    "        conv2 = nn.Conv2d(1, 128, kernel_size = (8,21), stride = 1, dilation = 1)\n",
    "        nn.init.xavier_uniform_(conv2.weight) \n",
    "        conv2.bias.data.fill_(bias_data)\n",
    "        \n",
    "        batch_conv2 = nn.BatchNorm2d(128, momentum = momentum_value, eps = eps_value)\n",
    "        pool2 = nn.MaxPool2d(kernel_size=(993,1)) \n",
    "        \n",
    "        self.CNN2_module = nn.Sequential(\n",
    "            conv2,\n",
    "            batch_conv2,\n",
    "            nn.ReLU(),\n",
    "            pool2\n",
    "        )\n",
    "        \n",
    "        conv3 = nn.Conv2d(1, 128, kernel_size = (16,21), stride = 1 , dilation = 1)\n",
    "        nn.init.xavier_uniform_(conv3.weight)\n",
    "        conv3.bias.data.fill_(bias_data)\n",
    "        \n",
    "        batch_conv3 = nn.BatchNorm2d(128, momentum = momentum_value, eps = eps_value)\n",
    "        pool3 = nn.MaxPool2d(kernel_size=(985,1)) \n",
    "        \n",
    "        self.CNN3_module = nn.Sequential(\n",
    "            conv3,\n",
    "            batch_conv3,\n",
    "            nn.ReLU(),\n",
    "            pool3\n",
    "        )\n",
    "        \n",
    "        conv4 = nn.Conv2d(1, 128, kernel_size = (21,21), stride = 1 , dilation = 1)\n",
    "        nn.init.xavier_uniform_(conv4.weight)\n",
    "        conv4.bias.data.fill_(bias_data)\n",
    "        \n",
    "        batch_conv4 = nn.BatchNorm2d(128, momentum = momentum_value, eps = eps_value)\n",
    "        pool4 = nn.MaxPool2d(kernel_size=(980,1)) \n",
    "        \n",
    "        self.CNN4_module = nn.Sequential(\n",
    "            conv4,\n",
    "            batch_conv4,\n",
    "            nn.ReLU(),\n",
    "            pool4\n",
    "        )\n",
    "        \n",
    "        fc1 = nn.Linear(384, 512) #케라스의 Dense\n",
    "        batch_fc1 = nn.BatchNorm1d(512, momentum = momentum_value, eps = eps_value)\n",
    "        nn.init.xavier_uniform_(fc1.weight) #케라스의 conv2d와 설정을 똑같이 하기위해 사용 \n",
    "        fc1.bias.data.fill_(bias_data) #케라스의 conv2d와 설정을 똑같이 하기위해 사용 \n",
    "        \n",
    "        fc2 = nn.Linear(512, 512)\n",
    "        batch_fc2 = nn.BatchNorm1d(512, momentum = momentum_value, eps = eps_value)\n",
    "        nn.init.xavier_uniform_(fc2.weight)\n",
    "        fc2.bias.data.fill_(bias_data)\n",
    "        \n",
    "        fc3 = nn.Linear(512, 2)\n",
    "        batch_fc3 = nn.BatchNorm1d(2, momentum = momentum_value, eps = eps_value)\n",
    "        nn.init.xavier_uniform_(fc3.weight)\n",
    "        fc3.bias.data.fill_(bias_data)\n",
    "        \n",
    "        self.fc_module = nn.Sequential(\n",
    "            fc1,\n",
    "            batch_fc1,\n",
    "            nn.ReLU(),\n",
    "            fc2,\n",
    "            batch_fc2,\n",
    "            nn.ReLU(),\n",
    "            fc3,\n",
    "            batch_fc3,\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.CNN1_module(x) # @16*4*4\n",
    "        out2 = self.CNN2_module(x) # @16*4*4\n",
    "        out3 = self.CNN3_module(x) # @16*4*4\n",
    "        out4 = self.CNN4_module(x) # @16*4*4\n",
    "        # make linear\n",
    "        \n",
    "        out = torch.cat((out1, out2, out3), dim = 1) # torch.cat은 케라스의 Concatenate\n",
    "        dim = 1\n",
    "        \n",
    "        for d in out.size()[1:]: # 최종 갯수를 알기위한 for문\n",
    "            dim = dim * d\n",
    "        \n",
    "        \n",
    "        out = out.view(-1, dim)\n",
    "        out = self.fc_module(out)\n",
    "        return out\n",
    "    \n",
    "#배치 정규화 균등분포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN1().cuda()\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | trn loss: 0.006287 test loss: 0.004995\n",
      "Pred Total_acc : 90.52, Non_acc : 0.00, Enz : 100.00\n",
      "작업 수행된 시간 : 365.115139 초\n",
      "\n",
      "epoch: 2 | trn loss: 0.004380 test loss: 0.004015\n",
      "Pred Total_acc : 90.52, Non_acc : 0.00, Enz : 100.00\n",
      "작업 수행된 시간 : 364.745459 초\n",
      "\n",
      "epoch: 3 | trn loss: 0.003804 test loss: 0.003734\n",
      "Pred Total_acc : 90.52, Non_acc : 0.00, Enz : 100.00\n",
      "작업 수행된 시간 : 364.254409 초\n",
      "\n",
      "epoch: 4 | trn loss: 0.003641 test loss: 0.003663\n",
      "Pred Total_acc : 90.52, Non_acc : 0.00, Enz : 100.00\n",
      "작업 수행된 시간 : 363.982031 초\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\voc\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-8-19f375edc401>\", line 48, in <module>\n",
      "    train_loss += loss.item()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\voc\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\voc\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1151, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\voc\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\voc\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\voc\\anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\voc\\anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\voc\\anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\voc\\anaconda3\\lib\\inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"C:\\Users\\voc\\anaconda3\\lib\\inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"C:\\Users\\voc\\anaconda3\\lib\\inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"C:\\Users\\voc\\anaconda3\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "class_weight_value = torch.FloatTensor(class_weight).cuda()\n",
    "\n",
    "criterion = nn.BCELoss(weight = class_weight_value)\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(cnn.parameters(), lr = learning_rate, eps = 1e-08)\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "train_loss_list = []\n",
    "test_loss_list = [] \n",
    "acc_list = [] \n",
    "non_acc_list = []\n",
    "enz_acc_list = []\n",
    "check_zero_pred_correct = 0\n",
    "\n",
    "\n",
    "for e in range(epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    test_loss = 0\n",
    "    \n",
    "    test_zero_count = 0\n",
    "    test_one_count = 0\n",
    "    \n",
    "    total_pred_correct = 0\n",
    "    one_pred_correct = 0\n",
    "    zero_pred_correct = 0\n",
    "    \n",
    "    train_data = data_loader(train_length, train_seq, train_label)\n",
    "    test_data = data_loader(test_length, test_seq, test_label)\n",
    "    val_data = data_loader(val_length, val_seq, val_label)\n",
    "    \n",
    "    for index, data in enumerate(traindataloader):\n",
    "        cnn.train()\n",
    "        input_data, label = data\n",
    "       \n",
    "        input_data = input_data.cuda(non_blocking = True)\n",
    "        trian_label = label.cuda(non_blocking = True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        model_output = cnn(input_data)\n",
    "        \n",
    "        loss = criterion(model_output, trian_label)\n",
    "        loss.backward()          \n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    with torch.no_grad(): # very very very very important!!!\n",
    "        \n",
    "        total_pred_correct = 0\n",
    "        zero_pred_correct = 0\n",
    "        one_pred_correct = 0\n",
    "        \n",
    "        for count, testdata in enumerate(testdataloader):\n",
    "            cnn.eval()\n",
    "            \n",
    "            input_data, test_labels = testdata\n",
    "            test_input_data = input_data.cuda(non_blocking = True)\n",
    "            test_labels = test_labels.cuda(non_blocking = True)\n",
    "            \n",
    "            test_outputs = cnn(test_input_data)\n",
    "\n",
    "            _, total_pred = torch.max(test_outputs.data, 1)\n",
    "            \n",
    "            loss = criterion(test_outputs, test_labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            for t in range(test_batch_size_value):\n",
    "                if test_labels[t][0] ==  1:\n",
    "                    test_zero_count += 1\n",
    "                    if total_pred[t] == 0:\n",
    "                        total_pred_correct += 1\n",
    "                        zero_pred_correct += 1\n",
    "                        \n",
    "                elif test_labels[t][0] ==  0:\n",
    "                    test_one_count += 1\n",
    "                    if total_pred[t] == 1:\n",
    "                        total_pred_correct += 1\n",
    "                        one_pred_correct += 1\n",
    "        \n",
    "        check_zero_correct = float(100 * zero_pred_correct) / test_zero_count\n",
    "        check_one_correct = float(100 * one_pred_correct) / test_one_count\n",
    "        \n",
    "        if check_zero_correct > 80:\n",
    "            if check_one_correct > 90:\n",
    "                torch.save(cnn.state_dict(), './saved_model/cnn1_softmax.pth')\n",
    "    \n",
    "        train_loss_list.append(train_loss/train_length)\n",
    "        test_loss_list.append(test_loss/test_length)\n",
    "        acc_list.append(float(100 * total_pred_correct) / test_length)\n",
    "        non_acc_list.append(float(100 * zero_pred_correct) / test_zero_count)\n",
    "        enz_acc_list.append(float(100 * one_pred_correct) / test_one_count)\n",
    "        \n",
    "    print(\"epoch: {} | trn loss: {:.6f} test loss: {:.6f}\".format(e+1, train_loss/train_length, test_loss / test_length))\n",
    "    print('Pred Total_acc : {:.2f}, Non_acc : {:.2f}, Enz : {:.2f}'.format(float(100 * total_pred_correct) / test_length, float(100 * zero_pred_correct) / test_zero_count, float(100 * one_pred_correct) / test_one_count))\n",
    "    print('작업 수행된 시간 : %f 초' % (time.time() - start_time))\n",
    "  \n",
    "    print(\"\")\n",
    "# 학습 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model_output[0][0]\n",
    "b = model_output[0][1]\n",
    "\n",
    "c = [a,b]\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = nn.Sigmoid()\n",
    "softmax = nn.Softmax(dim = 1)\n",
    "sof = softmax(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6357, device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor(0.6090, device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([0.5568, 0.4432], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([0.6291, 0.6158], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([0.5067, 0.4933], device='cuda:0', grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid(a))\n",
    "print(sigmoid(b))\n",
    "\n",
    "t = sigmoid(model_output)\n",
    "print(model_output[0])\n",
    "print(sigmoid(sof[0]))\n",
    "\n",
    "print(softmax(t)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
